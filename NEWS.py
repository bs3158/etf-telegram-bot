import os
import feedparser
import requests
from bs4 import BeautifulSoup
import re
from googletrans import Translator # ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# í™˜ê²½ ë³€ìˆ˜
BOT_TOKEN = os.environ.get("BOT_TOKEN")
CHAT_ID = os.environ.get("CHAT_ID")

RSS_LIST = [
    "https://www.hankyung.com/feed/economy",
    "https://www.mk.co.kr/rss/30000001/",
    "https://www.cnbc.com/id/10001147/device/rss/rss.html" # ì˜ì–´ ì†ŒìŠ¤
]

translator = Translator()

def translate_text(text):
    try:
        # í…ìŠ¤íŠ¸ê°€ ì˜ì–´ì¸ì§€ í™•ì¸ í›„ í•œêµ­ì–´ë¡œ ë²ˆì—­
        result = translator.translate(text, dest='ko')
        return result.text
    except:
        return text # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë¬¸ ë°˜í™˜

def get_summary(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        r = requests.get(url, timeout=5, headers=headers)
        soup = BeautifulSoup(r.text, "html.parser")
        for s in soup(['script', 'style', 'header', 'footer', 'nav']):
            s.decompose()
        text = soup.get_text(" ", strip=True)
        sentences = re.split(r'(?<=[.!?])\s+', text)
        valid_sentences = [s for s in sentences if len(s) > 30 and len(s) < 200]
        summary = " ".join(valid_sentences[:2])
        return summary if summary else "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except:
        return "ìš”ì•½ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def collect_and_send():
    all_news = []
    for rss in RSS_LIST:
        feed = feedparser.parse(rss)
        # ì†ŒìŠ¤ë‹¹ ê¸°ì‚¬ë¥¼ ì¶©ë¶„íˆ ê°€ì ¸ì˜¨ ë’¤ ë‚˜ì¤‘ì— 20ê°œë¡œ ìë¦…ë‹ˆë‹¤.
        for entry in feed.entries[:10]: 
            all_news.append({"title": entry.title, "link": entry.link})

    # ì´ 20ê°œë¡œ ì œí•œí•˜ë˜, ìˆ˜ì§‘ëœ ê²Œ 20ê°œë³´ë‹¤ ì ì„ ìƒí™©ë„ ëŒ€ë¹„í•©ë‹ˆë‹¤.
    target_news = all_news[:20]
    total_articles = len(target_news)
    
    # 5ê°œì”© ë¬¶ì„ ë•Œ ì´ ëª‡ ê°œì˜ ë©”ì‹œì§€ê°€ ìƒì„±ë ì§€ ë¯¸ë¦¬ ê³„ì‚°
    chunk_size = 5
    # total_chunks ê³„ì‚°: (ì „ì²´ ê°œìˆ˜ + 4) // 5 ë°©ì‹ (ì˜¬ë¦¼ ì²˜ë¦¬)
    total_chunks = (total_articles + chunk_size - 1) // chunk_size

    for i in range(0, total_articles, chunk_size):
        chunk = target_news[i:i+chunk_size]
        current_chunk_num = (i // chunk_size) + 1
        
        # ìƒë‹¨ í‘œê¸°: [í˜„ì¬ ë²ˆí˜¸ / ì „ì²´ ë²ˆí˜¸]
        message = f"<b>ğŸš€ ì‹¤ì‹œê°„ ì£¼ìš” ë‰´ìŠ¤ ({current_chunk_num}/{total_chunks})</b>\n\n"
        
        for idx, item in enumerate(chunk):
            title = item['title']
            summary = get_summary(item['link'])
            
            # ì˜ë¬¸ ë‰´ìŠ¤ ìë™ ê°ì§€ ë° ë²ˆì—­ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            if re.search('[a-zA-Z]{5,}', title): # ì•ŒíŒŒë²³ 5ì ì´ìƒ ì—°ì† ì‹œ ì˜ì–´ë¡œ ê°„ì£¼
                title = f"[ë²ˆì—­] " + translate_text(title)
                summary = translate_text(summary)

            message += f"<b>{idx+1}. {title}</b>\n"
            message += f"ğŸ“ {summary}\n"
            message += f"ğŸ”— <a href='{item['link']}'>ê¸°ì‚¬ ë³´ê¸°</a>\n\n"
            message += "--------------------------\n\n"
        
        send_to_telegram(message)

def send_to_telegram(text):
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {"chat_id": CHAT_ID, "text": text, "parse_mode": "HTML", "disable_web_page_preview": True}
    requests.post(url, data=payload)

if __name__ == "__main__":
    collect_and_send()
