import os
import feedparser
import requests
from bs4 import BeautifulSoup
import re
from googletrans import Translator

# í™˜ê²½ ë³€ìˆ˜
BOT_TOKEN = os.environ.get("BOT_TOKEN")
CHAT_ID = os.environ.get("CHAT_ID")

# 4ê°œì˜ ì‚¬ì´íŠ¸ ì„¤ì •
RSS_LIST = [
    "https://www.hani.co.kr/rss/",    # í•œê²¨ë ˆ ê²½ì œ
    "https://www.hankyung.com/feed/economy",   # í•œêµ­ê²½ì œ
    "https://www.mk.co.kr/rss/30000001/",      # ë§¤ì¼ê²½ì œ
    "http://rss.cnn.com/rss/edition_business.rss" # ìµœì‹  CNN Business RSS
]

translator = Translator()

def translate_text(text):
    try:
        if not text or text.strip() == "": return text
        result = translator.translate(text, dest='ko')
        return result.text
    except:
        return text

def get_summary(url):
    try:
        # CNN ë° êµ­ë‚´ ì–¸ë¡ ì‚¬ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ë¸Œë¼ìš°ì € í—¤ë” ê°•í™”
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        r = requests.get(url, timeout=10, headers=headers)
        r.encoding = 'utf-8'
        
        if r.status_code != 200:
            return "ë³¸ë¬¸ ìš”ì•½ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì ‘ê·¼ ê¶Œí•œ ì œí•œ)"

        soup = BeautifulSoup(r.text, "html.parser")

        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for s in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'form']):
            s.decompose()

        # CNN ì „ìš© ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„ (í´ë˜ìŠ¤ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë³‘í–‰)
        content = soup.find('div', {'class': 'article__content'}) or soup.find('section', {'class': 'layout__wrapper'})
        if content:
            text = content.get_text(" ", strip=True)
        else:
            text = soup.get_text(" ", strip=True)

        sentences = re.split(r'(?<=[.!?])\s+', text)
        # ë³¸ë¬¸ í•µì‹¬ ë¬¸ì¥ í•„í„°ë§ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ ë¬¸ì¥)
        valid_sentences = [s for s in sentences if 50 < len(s) < 250]
        
        summary = " ".join(valid_sentences[:2])
        return summary if summary else "ë³¸ë¬¸ ë‚´ìš©ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except:
        return "ìš”ì•½ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def collect_and_send():
    all_chunks = []

    for rss_url in RSS_LIST:
        feed = feedparser.parse(rss_url)
        source_news = []
        # ê° ì‚¬ì´íŠ¸ì—ì„œ ìƒìœ„ 5ê°œ ì¶”ì¶œ
        for entry in feed.entries[:5]:
            source_news.append({
                "title": entry.title,
                "link": entry.link
            })
        all_chunks.append(source_news)

    for i, chunk in enumerate(all_chunks):
        current_num = i + 1
        source_name = ["í•œê²¨ë ˆ", "í•œêµ­ê²½ì œ", "ë§¤ì¼ê²½ì œ", "CNN(í•´ì™¸)"][i]

        message = f"<b>ğŸš€ ì‹¤ì‹œê°„ ì£¼ìš” ë‰´ìŠ¤ ({current_num}/4) - {source_name}</b>\n\n"

        for idx, item in enumerate(chunk):
            title = item['title']
            summary = get_summary(item['link'])

            # 4ë²ˆì§¸ ì†ŒìŠ¤(CNN)ì´ê±°ë‚˜ ì œëª©ì— ì˜ì–´ê°€ ë§ìœ¼ë©´ ë²ˆì—­
            if current_num == 4 or re.search('[a-zA-Z]{7,}', title):
                title = f"[ë²ˆì—­] " + translate_text(title)
                summary = translate_text(summary)

            message += f"<b>{idx+1}. {title}</b>\n"
            message += f"ğŸ“ {summary}\n"
            message += f"ğŸ”— <a href='{item['link']}'>ê¸°ì‚¬ ë³´ê¸°</a>\n\n"
            message += "--------------------------\n\n"

        send_to_telegram(message)

def send_to_telegram(text):
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }
    try:
        requests.post(url, data=payload, timeout=10)
    except:
        pass

if __name__ == "__main__":
    collect_and_send()
